{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import r2_score\n",
    "from skimage import io,transform\n",
    "from torchvision import transforms, utils\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    label_dir = \"./Label_5p.csv\"\n",
    "    image_dir = \"./data/ROI_trab\"\n",
    "    train_cross = \"./cross_output.pkl\"\n",
    "    batch_size = 8\n",
    "    model = \"ConvNet\" \n",
    "    nof = 8\n",
    "    lr= 0.001\n",
    "    nb_epochs = 5\n",
    "    checkpoint_path= \"./\"\n",
    "    mode= \"Train\"\n",
    "    cross_val = False\n",
    "    k_fold= 5\n",
    "    n1= 240\n",
    "    n2= 120\n",
    "    n3 = 60\n",
    "    nb_workers = 0\n",
    "    norm_method=\"standardization\"\n",
    "\n",
    "opt = Args()\n",
    "NB_DATA = 3991\n",
    "NB_LABEL = 5\n",
    "PERCENTAGE_TEST = 20\n",
    "RESIZE_IMAGE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-12 10:44:13,503]\u001b[0m A new study created in memory with name: no-name-b90ec92d-6fc3-481e-ab78-cef1a4c28de9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(), direction='minimize')\n",
    "\n",
    "\n",
    "NB_DATA = 3991\n",
    "NB_LABEL = 5\n",
    "PERCENTAGE_TEST = 20\n",
    "RESIZE_IMAGE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, opt, indices,transform=None):\n",
    "        self.opt = opt\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.image_dir, str(self.labels.iloc[idx,0]))\n",
    "        image = io.imread(img_name) # Loading Image\n",
    "        image = image / 255.0 # Normalizing [0;1]\n",
    "        image = image.astype('float32') # Converting images to float32\n",
    "        labels = self.labels.iloc[idx,1:] # Takes all corresponding labels\n",
    "        labels = np.array([labels]) \n",
    "        labels = labels.astype('float32')\n",
    "        return {\"image\":image,\"label\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,n1,n2,n3,out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(64*64*64,n1)\n",
    "        self.fc2 = nn.Linear(n1,n2)\n",
    "        self.fc3 = nn.Linear(n2,n3)\n",
    "        #self.fc5 = nn.Linear(n3,20)\n",
    "        self.fc4 = nn.Linear(n3,out_channels)\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #x = F.relu(self.fc5(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,features,out_channels,n1=240,n2=120,n3=60,k1=3,k2=3,k3=3):\n",
    "        super(ConvNet,self).__init__()\n",
    "        # initialize CNN layers \n",
    "        self.conv1 = nn.Conv2d(1,features,kernel_size = k1,stride = 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(features,features*2, kernel_size = k2, stride = 1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(features*2,64, kernel_size = k3, stride = 1, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        # initialize NN layers\n",
    "        #self.fc1 = nn.Linear(64**3,n1)\n",
    "        #self.fc2 = nn.Linear(n1,n2)\n",
    "        #self.fc3 = nn.Linear(n2,14)\n",
    "        self.neural = NeuralNet(n1,n2,n3,out_channels)\n",
    "        # dropout\n",
    "        # self.dropout = nn.Dropout(0.25)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.neural(x)\n",
    "        #x = torch.flatten(x,1)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader, optimizer, epoch ,steps_per_epochs=20):\n",
    "        model.train()\n",
    "        print(\"starting training\")\n",
    "        print(\"----------------\")\n",
    "        train_loss = 0.0\n",
    "        train_total = 0\n",
    "        running_loss = 0.0\n",
    "        r2_s = 0\n",
    "        mse_score = 0.0\n",
    "        for i, data in enumerate(trainloader,0):\n",
    "            inputs, labels = data['image'], data['label']\n",
    "            # reshape\n",
    "            inputs = inputs.reshape(inputs.size(0),1,512,512)\n",
    "            labels = labels.reshape(labels.size(0),NB_LABEL)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward backward and optimization\n",
    "            outputs = model(inputs)\n",
    "            print(outputs.shape)\n",
    "            print(labels.shape)\n",
    "            Loss = MSELoss()\n",
    "            loss = Loss(outputs,labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # statistics\n",
    "            train_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "            train_total += labels.size(0)\n",
    "            outputs, labels = outputs.cpu().detach().numpy(), labels.cpu().detach().numpy()\n",
    "            labels, outputs = np.array(labels), np.array(outputs)\n",
    "            labels, outputs = labels.reshape(NB_LABEL,len(inputs)), outputs.reshape(NB_LABEL,len(inputs))\n",
    "            r2 = r2_score(labels,outputs)\n",
    "            r2_s += r2\n",
    "            Loss = MSELoss()\n",
    "            mse_score += Loss(outputs,labels)\n",
    "            if i % opt['batch_size'] == opt['batch_size']-1:\n",
    "                print('[%d %5d], loss: %.3f' %\n",
    "                      (epoch + 1, i+1, running_loss/opt['batch_size']))\n",
    "                running_loss = 0.0\n",
    "        # displaying results\n",
    "        mse = mse_score / i\n",
    "        r2_s = r2_s/i\n",
    "        print('Epoch [{}], Loss: {}, R square: {}'.format(epoch+1, train_loss/train_total, r2_s), end='')\n",
    "        print('Finished Training')\n",
    "        \n",
    "        return mse\n",
    "\n",
    "def test(model,testloader,epoch):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    test_total = 0\n",
    "    r2_s = 0\n",
    "    mse_score = 0.0\n",
    "    output = {}\n",
    "    label = {}\n",
    "    # Loading Checkpoint\n",
    "    if opt['mode'] == \"Test\":\n",
    "        check_name = \"BPNN_checkpoint_\" + str(epoch) + \".pth\"\n",
    "        model.load_state_dict(torch.load(os.path.join(opt['checkpoint_path'],check_name)))\n",
    "    # Testing\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data['image'],data['label']\n",
    "            # reshape\n",
    "            inputs = inputs.reshape(1,1,512,512)\n",
    "            labels = labels.reshape(1,NB_LABEL)\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            # loss\n",
    "            outputs = model(inputs)\n",
    "            Loss = MSELoss()\n",
    "            test_loss += Loss(outputs,labels)\n",
    "            test_total += labels.size(0)\n",
    "            # statistics\n",
    "            outputs, labels = outputs.cpu().detach().numpy(), labels.cpu().detach().numpy()\n",
    "            labels, outputs = np.array(labels), np.array(outputs)\n",
    "            labels, outputs = labels.reshape(NB_LABEL,1), outputs.reshape(NB_LABEL,1)\n",
    "            r2 = r2_score(labels,outputs)\n",
    "            r2_s += r2\n",
    "            Loss = MSELoss()\n",
    "            mse_score += Loss(outputs,labels)\n",
    "\n",
    "            outputs,labels=outputs.reshape(1,NB_LABEL), labels.reshape(1,NB_LABEL)\n",
    "            output[i] = outputs\n",
    "            label[i] = labels\n",
    "        name_out = \"./output\" + str(epoch) + \".txt\"\n",
    "        name_lab = \"./label\" + str(epoch) + \".txt\"\n",
    "        mse = mse_score/i\n",
    "\n",
    "\n",
    "    r2_s = r2_s/i\n",
    "    print(' Test_loss: {}, Test_R_square: {}'.format(test_loss/test_total, r2_s))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Create the folder where to save results and checkpoints\n",
    "    mse_train = []\n",
    "    mse_test = []\n",
    "    opt = {'label_dir' : \"./Label_5p.csv\",\n",
    "           'image_dir' : \"./data/ROI_trab\",\n",
    "           'train_cross' : \"./cross_output.pkl\",\n",
    "           'batch_size' : 8,\n",
    "           'model' : \"ConvNet\",\n",
    "           'nof' : 8,\n",
    "           'lr': trial.suggest_loguniform('lr',1e-3,1e-2),\n",
    "           'nb_epochs' : 5,\n",
    "           'checkpoint_path' : \"./\",\n",
    "           'mode': \"Train\",\n",
    "           'cross_val' : False,\n",
    "           'k_fold' : 5,\n",
    "           'n1' : 240,\n",
    "           'n2' : 120,\n",
    "           'n3' : 60,\n",
    "           'nb_workers' : 0,\n",
    "           'norm_method':\"standardization\",\n",
    "           'optimizer' :  trial.suggest_categorical('optimizer',[SGD, Adam])\n",
    "\n",
    "          }\n",
    "    \n",
    "    # defining data\n",
    "    index = range(NB_DATA)\n",
    "    split = train_test_split(index,test_size = 0.2,random_state=1)\n",
    "    datasets = Datasets(csv_file = opt['label_dir'], image_dir = opt['image_dir'], opt=opt, indices = split[0]) # Create dataset\n",
    "    print(\"start training\")\n",
    "    trainloader = DataLoader(datasets, batch_size = opt['batch_size'], sampler = split[0], num_workers = opt['nb_workers'] )\n",
    "    testloader =DataLoader(datasets, batch_size = 1, sampler = split[1], num_workers = opt['nb_workers'] )\n",
    "    model = ConvNet(features =opt['nof'],out_channels=NB_LABEL,k1 = 3,k2 = 3,k3= 3).to(device)\n",
    "    model.apply(reset_weights)\n",
    "    optimizer = opt['optimizer'](model.parameters(), lr=opt['lr'])\n",
    "    for epoch in range(opt['nb_epochs']):\n",
    "        mse_train.append(train(model = model, trainloader = trainloader,optimizer = optimizer,epoch = epoch))\n",
    "        mse_test.append(test(model=model,testloader=testloader,epoch=epoch))\n",
    "    return max(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genlhc01/uki75tv/.local/lib/python3.7/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/linkhome/rech/genlhc01/uki75tv/.local/lib/python3.7/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset trainable parameters of layer = Linear(in_features=262144, out_features=240, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=240, out_features=120, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=120, out_features=60, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=60, out_features=5, bias=True)\n",
      "Reset trainable parameters of layer = Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "starting training\n",
      "----------------\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\"\n",
    "  print(\"running on gpu\")\n",
    "else:  \n",
    "  device = \"cpu\"\n",
    "  print(\"running on cpu\")\n",
    "    \n",
    "study.optimize(objective,n_trials=20)\n",
    "joblid.dump(study,'./train_optuna.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
