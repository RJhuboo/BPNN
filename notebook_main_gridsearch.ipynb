{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam, SGD\n",
    "from sklearn.metrics import r2_score\n",
    "from skimage import io,transform\n",
    "from torchvision import transforms, utils\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import optuna\n",
    "import joblib\n",
    "from math import isnan\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalization(csv_file,mode,indices):\n",
    "    Data = pd.read_csv(csv_file)\n",
    "    if mode == \"standardization\":\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "    elif mode == \"minmax\":\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "    scaler.fit(Data.iloc[indices,1:])\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    label_dir = \"./Label_5p.csv\"\n",
    "    image_dir = \"./data/ROI_trab\"\n",
    "    train_cross = \"./cross_output.pkl\"\n",
    "    batch_size = 8\n",
    "    model = \"ConvNet\" \n",
    "    nof = 8\n",
    "    lr= 0.001\n",
    "    nb_epochs = 5\n",
    "    checkpoint_path= \"./\"\n",
    "    mode= \"Train\"\n",
    "    cross_val = False\n",
    "    k_fold= 5\n",
    "    n1= 240\n",
    "    n2= 120\n",
    "    n3 = 60\n",
    "    nb_workers = 0\n",
    "    norm_method=\"standardization\"\n",
    "\n",
    "opt = Args()\n",
    "NB_DATA = 3991\n",
    "NB_LABEL = 5\n",
    "PERCENTAGE_TEST = 20\n",
    "RESIZE_IMAGE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-15 14:53:16,628]\u001b[0m A new study created in memory with name: no-name-cac05c0f-964a-4ac3-abb5-30cfea1cb642\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(sampler=optuna.samplers.TPESampler(),\n",
    "                            pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=2, n_warmup_steps=5, interval_steps=3\n",
    "    ),\n",
    "                            direction='minimize')\n",
    "\n",
    "\n",
    "NB_DATA = 3991\n",
    "NB_LABEL = 5\n",
    "PERCENTAGE_TEST = 20\n",
    "RESIZE_IMAGE = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, opt, indices,transform=None):\n",
    "        self.opt = opt\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.indices = indices\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        img_name = os.path.join(self.image_dir, str(self.labels.iloc[idx,0]))\n",
    "        image = io.imread(img_name) # Loading Image\n",
    "        image = image / 255.0 # Normalizing [0;1]\n",
    "        image = image.astype('float32') # Converting images to float32\n",
    "        if self.opt['norm_method']== \"L2\":\n",
    "            lab = preprocessing.normalize(self.labels.iloc[:,1:],axis=0)\n",
    "        elif self.opt['norm_method'] == \"L1\":\n",
    "            lab = preprocessing.normalize(self.labels.iloc[:,1:],norm='l1',axis=0)\n",
    "        elif self.opt['norm_method'] == \"minmax\":\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            scaler.fit(self.labels.iloc[self.indices,1:])\n",
    "            lab = scaler.transform(self.labels.iloc[:,1:])\n",
    "        elif self.opt['norm_method'] == \"standardization\":\n",
    "            scaler = preprocessing.StandardScaler()\n",
    "            scaler.fit(self.labels.iloc[self.indices,1:])\n",
    "            lab = scaler.transform(self.labels.iloc[:,1:])\n",
    "        lab = pd.DataFrame(lab)\n",
    "        lab.insert(0,\"File name\", self.labels.iloc[:,0], True)\n",
    "        lab.columns = self.labels.columns\n",
    "        labels = lab.iloc[idx,1:] # Takes all corresponding labels\n",
    "        labels = np.array([labels]) \n",
    "        labels = labels.astype('float32')\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return {'image': image, 'label': labels}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,activation,n1,n2,n3,out_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(64*64*64,n1)\n",
    "        self.fc2 = nn.Linear(n1,n2)\n",
    "        self.fc3 = nn.Linear(n2,n3)\n",
    "        self.fc4 = nn.Linear(n3,out_channels)\n",
    "        self.activation = activation\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,activation, features,out_channels,n1=240,n2=120,n3=60,k1=3,k2=3,k3=3):\n",
    "        super(ConvNet,self).__init__()\n",
    "        # initialize CNN layers \n",
    "        self.conv1 = nn.Conv2d(1,features,kernel_size = k1,stride = 1, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(features,features*2, kernel_size = k2, stride = 1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(features*2,64, kernel_size = k3, stride = 1, padding = 1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.activation = activation\n",
    "        # initialize NN layers\n",
    "        self.neural = NeuralNet(activation,n1,n2,n3,out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.activation(self.conv1(x)))\n",
    "        x = self.pool(self.activation(self.conv2(x)))\n",
    "        x = self.pool(self.activation(self.conv3(x)))\n",
    "        x = self.neural(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        print(f'Reset trainable parameters of layer = {layer}')\n",
    "        layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,trainloader, optimizer, epoch , opt, steps_per_epochs=20):\n",
    "    model.train()\n",
    "    print(\"starting training\")\n",
    "    print(\"----------------\")\n",
    "    train_loss = 0.0\n",
    "    train_total = 0\n",
    "    running_loss = 0.0\n",
    "    r2_s = 0\n",
    "    mse_score = 0.0\n",
    "    for i, data in enumerate(trainloader,0):\n",
    "        inputs, labels = data['image'], data['label']\n",
    "        # reshape\n",
    "        inputs = inputs.reshape(inputs.size(0),1,512,512)\n",
    "        labels = labels.reshape(labels.size(0),NB_LABEL)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward backward and optimization\n",
    "        outputs = model(inputs)\n",
    "        Loss = MSELoss()\n",
    "        loss = Loss(outputs,labels)\n",
    "        if isnan(loss) == True:\n",
    "            print(outputs)\n",
    "            print(labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # statistics\n",
    "        train_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        train_total += labels.size(0)\n",
    "        outputs, labels = outputs.cpu().detach().numpy(), labels.cpu().detach().numpy()\n",
    "        labels, outputs = np.array(labels), np.array(outputs)\n",
    "        labels, outputs = labels.reshape(NB_LABEL,len(inputs)), outputs.reshape(NB_LABEL,len(inputs))\n",
    "        #Loss = MSELoss()\n",
    "        mse_score += loss\n",
    "        if i % opt['batch_size'] == opt['batch_size']-1:\n",
    "            print('[%d %5d], loss: %.3f' %\n",
    "                  (epoch + 1, i+1, running_loss/opt['batch_size']))\n",
    "            running_loss = 0.0\n",
    "    # displaying results\n",
    "    mse = mse_score / i\n",
    "    print('Epoch [{}], Loss: {}'.format(epoch+1, train_loss/train_total), end='')\n",
    "    print('Finished Training')\n",
    "\n",
    "    return mse\n",
    "\n",
    "def test(model,testloader,epoch,opt):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    test_total = 0\n",
    "    r2_s = 0\n",
    "    mse_score = 0.0\n",
    "    output = {}\n",
    "    label = {}\n",
    "    # Loading Checkpoint\n",
    "    if opt['mode'] == \"Test\":\n",
    "        check_name = \"BPNN_checkpoint_\" + str(epoch) + \".pth\"\n",
    "        model.load_state_dict(torch.load(os.path.join(opt['checkpoint_path'],check_name)))\n",
    "    # Testing\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            inputs, labels = data['image'],data['label']\n",
    "            # reshape\n",
    "            inputs = inputs.reshape(1,1,512,512)\n",
    "            labels = labels.reshape(1,NB_LABEL)\n",
    "            inputs, labels = inputs.to(device),labels.to(device)\n",
    "            # loss\n",
    "            outputs = model(inputs)\n",
    "            Loss = MSELoss()\n",
    "            test_loss += Loss(outputs,labels)\n",
    "            test_total += labels.size(0)\n",
    "            # statistics\n",
    "            outputs, labels = outputs.cpu().detach().numpy(), labels.cpu().detach().numpy()\n",
    "            labels, outputs = np.array(labels), np.array(outputs)\n",
    "            labels, outputs = labels.reshape(NB_LABEL,1), outputs.reshape(NB_LABEL,1)\n",
    "            #Loss = MSELoss()\n",
    "            mse_score += test_loss\n",
    "\n",
    "            outputs,labels=outputs.reshape(1,NB_LABEL), labels.reshape(1,NB_LABEL)\n",
    "            output[i] = outputs\n",
    "            label[i] = labels\n",
    "        name_out = \"./output\" + str(epoch) + \".txt\"\n",
    "        name_lab = \"./label\" + str(epoch) + \".txt\"\n",
    "        mse = mse_score/i\n",
    "\n",
    "\n",
    "\n",
    "    print(' Test_loss: {}'.format(test_loss/test_total))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-be7f4d3a2280>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-be7f4d3a2280>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    mse_test = []*\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Create the folder where to save results and checkpoints\n",
    "    mse_train = []\n",
    "    mse_test = []\n",
    "    mse_total = np.zeros(opt['nb_epochs'])\n",
    "    opt = {'label_dir' : \"./Label_5p.csv\",\n",
    "           'image_dir' : \"./data/ROI_trab\",\n",
    "           'train_cross' : \"./cross_output.pkl\",\n",
    "           'batch_size' : trial.suggest_int('batch_size',8,32,step=8),\n",
    "           'model' : \"ConvNet\",\n",
    "           'nof' : trial.suggest_int('nof',8,64),\n",
    "           'lr': trial.suggest_loguniform('lr',1e-4,1e-2),\n",
    "           'nb_epochs' : 80,\n",
    "           'checkpoint_path' : \"./\",\n",
    "           'mode': \"Train\",\n",
    "           'cross_val' : False,\n",
    "           'k_fold' : 5,\n",
    "           'n1' : trial.suggest_int('n1', 100,300),\n",
    "           'n2' : trial.suggest_int('n2',100,300),\n",
    "           'n3' : trial.suggest_int('n2',100,300),\n",
    "           'nb_workers' : 4,\n",
    "           'norm_method': trial.suggest_categorical('norm_method',[\"standardization\",\"minmax\"]),\n",
    "           'optimizer' :  trial.suggest_categorical(\"optimizer\",[Adam, SGD]),\n",
    "           'activation' : trial.suggest_categorical(\"activation\", [F.relu])\n",
    "                                                    \n",
    "          }\n",
    "    \n",
    "    # defining data\n",
    "    index = range(NB_DATA)\n",
    "    split = train_test_split(index,test_size = 0.2,random_state=1)\n",
    "    kf = KFold(n_splits = opt['k_fold'], shuffle=True)\n",
    "    kf.get_n_splits(split[0])\n",
    "    print(\"start training\")\n",
    "    for train_index, test_index in kf.split(split[0]):\n",
    "        if opt['norm_method'] == \"standardization\" or opt['norm_method'] == \"minmax\":\n",
    "            scaler = normalization(opt['label_dir'],opt['norm_method'],train_index)\n",
    "        else:\n",
    "            scaler = None\n",
    "        datasets = Datasets(csv_file = opt['label_dir'], image_dir = opt['image_dir'], opt=opt, indices = train_index) # Create dataset\n",
    "        trainloader = DataLoader(datasets, batch_size = opt['batch_size'], sampler = train_index, num_workers = opt['nb_workers'])\n",
    "        testloader =DataLoader(datasets, batch_size = 1, sampler = test_index, num_workers = opt['nb_workers'])\n",
    "        model = ConvNet(activation = opt['activation'],features =opt['nof'],out_channels=NB_LABEL,n1=opt['n1'],n2=opt['n2'],n3=opt['n3'],k1 = 3,k2 = 3,k3= 3).to(device)\n",
    "        model.apply(reset_weights)\n",
    "        optimizer = opt['optimizer'](model.parameters(), lr=opt['lr'])\n",
    "        for epoch in range(opt['nb_epochs']):\n",
    "            mse_train.append(train(model = model, trainloader = trainloader,optimizer = optimizer,epoch = epoch,opt=opt))\n",
    "            mse_test.append(test(model=model,testloader=testloader,epoch=epoch,opt=opt))\n",
    "        mse_total = mse_total + np.array(mse_test)\n",
    "    mse_mean = mse_total / opt['k_fold']\n",
    "    i_min = np.where(mse_mean == np.min(mse_mean))\n",
    "    print('best epoch :', i_min[0][0]+1)\n",
    "    with open(\"./best_epoch.pkl\",\"wb\") as f:\n",
    "        pickle.dump(i_min[0][0],f)\n",
    "    return np.min(mse_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cpu\n",
      "start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/linkhome/rech/genlhc01/uki75tv/.local/lib/python3.7/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/linkhome/rech/genlhc01/uki75tv/.local/lib/python3.7/site-packages/optuna/distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset trainable parameters of layer = Linear(in_features=262144, out_features=240, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=240, out_features=120, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=120, out_features=60, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=60, out_features=5, bias=True)\n",
      "Reset trainable parameters of layer = Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Reset trainable parameters of layer = Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "starting training\n",
      "----------------\n",
      "torch.Size([8, 5])\n",
      "torch.Size([8, 5])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  device = \"cuda:0\"\n",
    "  print(\"running on gpu\")\n",
    "else:  \n",
    "  device = \"cpu\"\n",
    "  print(\"running on cpu\")\n",
    "    \n",
    "    \n",
    "study.optimize(objective,n_trials=2)\n",
    "with open(\"./train_optuna.pkl\",\"wb\") as f:\n",
    "    pickle.dump(study,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
